import base64

import pandas as pd
import numpy as np

from backend.Classes.NeuralNetworkFiles import Layer
from backend.Classes import HyperParameters

import matplotlib.pyplot as plt

from tensorflow import keras

# Create a list of core layers to select from.
list_Core = []

def getCore():
    return list_Core

## Input
Name = "Input"
Display_Name = "Input"
Definition = ["Input() is used to instantiate a Keras tensor.\nA Keras tensor is a symbolic tensor-like object, which we augment with certain attributes that allow us to build a Keras model just by knowing the inputs and outputs of the model.\n\nFor instance, if a, b and c are Keras tensors, it becomes possible to do: model = Model(input=[a, b], output=c)"]
Parameter_0 = {"Name":"shape_x", "Type": ["int"], "Default_option":"", "Default_value":"", "Possible":["int"],
             "Definition":"First axis of the shape of the input to the neural network.\nA shape tuple (integers), not including the batch size. For instance, shape=(32,) indicates that the expected input will be batches of 32-dimensional vectors. Elements of this tuple can be None; 'None' elements represent dimensions where the shape is not known."}
Parameter_1 = {"Name":"shape_y", "Type": ["int"], "Default_option":"", "Default_value":"", "Possible":["int"],
             "Definition":"Second axis shape of the input to the neural network.\n"}
Parameter_2 = {"Name":"shape_z", "Type": ["int"], "Default_option":"", "Default_value":"", "Possible":["int"],
             "Definition":"Third axis of the shape of the input to the neural network.\n"}
Parameter_3 = {"Name":"shape_w", "Type": ["int"], "Default_option":"", "Default_value":"", "Possible":["int"],
             "Definition":"Fourth axis of the shape of the input to the neural network.\n"}
Parameter_4 = {"Name":"batch_size", "Type": ["int"], "Default_option":"", "Default_value":"", "Possible":["int"],
             "Definition":"optional static batch size (integer)."}
Parameter_5 = {"Name":"name", "Type": ["str"], "Default_option":"", "Default_value":"", "Possible":["string"],
             "Definition":"An optional name string for the layer. Should be unique in a model (do not reuse the same name twice). It will be autogenerated if it isn't provided."}
Parameter_6 = {"Name":"dtype", "Type": ["option_dtype"], "Default_option":"None", "Default_value":"None", "Possible":["np.float64", "np.float32","np.int64", "np.int32","None"],
             "Definition":"The data type expected by the input, as a string (float32, float64, int32...)"}
Parameter_7 = {"Name":"sparse", "Type": ["bool"], "Default_option":"None", "Default_value":"None", "Possible":[True,False,"None"],
             "Definition":"A boolean specifying whether the placeholder to be created is sparse. Only one of 'ragged' and 'sparse' can be True. Note that, if sparse is False, sparse tensors can still be passed into the input - they will be densified with a default value of 0."}
## Removed as we are not allowing user to create a tensor then select to use.
#Parameter_0 = {"Name":"tensor", "Type": ["int"], "Default_option":"", "Default_value":"", "Possible":["int"],
#             "Definition":"Optional existing tensor to wrap into the Input layer. If set, the layer will use the tf.TypeSpec of this tensor rather than creating a new placeholder tensor."}
Parameter_8 = {"Name":"ragged", "Type": ["bool"], "Default_option":"None", "Default_value":"None", "Possible":[True,False,"None"],
             "Definition":"A boolean specifying whether the placeholder to be created is ragged. Only one of 'ragged' and 'sparse' can be True. In this case, values of 'None' in the 'shape' argument represent ragged dimensions."}
## Removed as we are not allowing user to create an object place holder then select to use.
#Parameter_0 = {"Name":"type_spec", "Type": ["int"], "Default_option":"", "Default_value":"", "Possible":["int"],
#             "Definition":"A tf.TypeSpec object to create the input placeholder from. When provided, all other args except name must be None."}


Parameters = {"Parameter_0":Parameter_0, "Parameter_1":Parameter_1, "Parameter_2":Parameter_2, "Parameter_3":Parameter_3,
              "Parameter_4":Parameter_4, "Parameter_5":Parameter_5, "Parameter_6":Parameter_6, "Parameter_7":Parameter_7,
              "Parameter_8":Parameter_8}

list_Core.append(Layer.Layer(Name, Display_Name, Definition, Parameters))

## Dense Layer
Name = "Dense"
Display_Name = "Dense"
Definition = ["Just your regular densely-connected NN layer.\n\nDense implements the operation: output = activation(dot(input, kernel) + bias) where activation is the element-wise activation function passed as the activation argument, kernel is a weights matrix created by the layer, and bias is a bias vector created by the layer (only applicable if use_bias is True). These are all attributes of Dense.\n\nNote: If the input to the layer has a rank greater than 2, then Dense computes the dot product between the inputs and the kernel along the last axis of the inputs and axis 0 of the kernel (using tf.tensordot). For example, if input has dimensions (batch_size, d0, d1), then we create a kernel with shape (d1, units), and the kernel operates along axis 2 of the input, on every sub-tensor of shape (1, 1, d1) (there are batch_size * d0 such sub-tensors). The output in this case will have shape (batch_size, d0, units).\n\nBesides, layer attributes cannot be modified after the layer has been called once (except the trainable attribute). When a popular kwarg input_shape is passed, then keras will create an input layer to insert before the current layer. This can be treated equivalent to explicitly defining an InputLayer."]
Parameter_0 = {"Name":"units", "Type": ["int"], "Default_option":"", "Default_value":"", "Possible":["int"],
             "Definition":"Positive integer, dimensionality of the output space."}
Parameter_1 = {"Name":"activation", "Type": ["option"], "Default_option":"None", "Default_value":"None", "Possible":["relu","sigmoid","softmax","softplus","softsign","tanh","selu","elu","exponential","None"],
             "Definition":"Activation function to use. If you don`t specify anything, no activation is applied (ie. 'linear' activation: a(x) = x)."}
Parameter_2 = {"Name":"use_bias", "Type": ["bool"], "Default_option":True, "Default_value":True, "Possible":[True,False],
             "Definition":"Boolean, whether the layer uses a bias vector."}
Parameter_3 = {"Name":"kernel_initializer", "Type": ["option"], "Default_option":"glorot_uniform", "Default_value":"glorot_uniform", "Possible":Layer.getInitializers(),
             "Definition":"Initializer for the kernel weights matrix."}
Parameter_4 = {"Name":"bias_initializer", "Type": ["option"], "Default_option":"zeros", "Default_value":"zeros", "Possible":Layer.getInitializers(),
             "Definition":"Initializer for the bias vector."}
Parameter_5 = {"Name":"kernel_regularizer", "Type": ["option"], "Default_option":"None", "Default_value":"None", "Possible":Layer.getRegularizers(),
             "Definition":"Regularizer function applied to the kernel weights matrix."}
Parameter_6 = {"Name":"bias_regularizer", "Type": ["option"], "Default_option":"None", "Default_value":"None", "Possible":Layer.getRegularizers(),
            "Definition":"Regularizer function applied to the bias vector."}
Parameter_7 = {"Name":"activity_regularizer", "Type": ["option"], "Default_option":"None", "Default_value":"None", "Possible":Layer.getRegularizers(),
             "Definition":"Regularizer function applied to the output of the layer (its 'activation')."}
## Removed till later as this would add another layer of options, which would require additional coding and complexity to the site.
## In otherwords it would require to check which is selected, then create another side box for the perameters of the select option for user to fill.
## then create a method to get these parameters.
#Parameter_8 = {"Name":"kernel_constraint", "Type": ["option"], "Default_option":"None", "Default_value":"None", "Possible":Layer.getConstraints(),
#             "Definition":"Constraint function applied to the kernel weights matrix."}
#Parameter_9 = {"Name":"bias_constraint", "Type": ["option"], "Default_option":"None", "Default_value":"None", "Possible":Layer.getConstraints(),
#             "Definition":"Constraint function applied to the bias vector."}

Parameters = {"Parameter_0":Parameter_0, "Parameter_1":Parameter_1, "Parameter_2":Parameter_2, "Parameter_3":Parameter_3,
              "Parameter_4":Parameter_4, "Parameter_5":Parameter_5, "Parameter_6":Parameter_6, "Parameter_7":Parameter_7}

list_Core.append(Layer.Layer(Name, Display_Name, Definition, Parameters))

## Activation Layer
Name = "Activation"
Display_Name = "Activation"
Definition = ["Applies an activation function to an output."]
Parameter_0 = {"Name":"units", "Type": ["option"], "Default_option":"None", "Default_value":"None", "Possible":Layer.getActivations(),
             "Definition":" Activation function, such as tf.nn.relu, or string name of built-in activation function, such as 'relu'."}

Parameters = {"Parameter_0":Parameter_0}

list_Core.append(Layer.Layer(Name, Display_Name, Definition, Parameters))

## Embedding Layer
Name = "Embedding"
Display_Name = "Embedding"
Definition = ["Turns positive integers (indexes) into dense vectors of fixed size.\n\ne.g. [[4], [20]] -> [[0.25, 0.1], [0.6, -0.2]]\n\nThis layer can only be used on positive integer inputs of a fixed range. The tf.keras.layers.TextVectorization, tf.keras.layers.StringLookup, and tf.keras.layers.IntegerLookup preprocessing layers can help prepare inputs for an Embedding layer.\n\nThis layer accepts tf.Tensor, tf.RaggedTensor and tf.SparseTensor input."]
Parameter_0 = {"Name":"input_dim", "Type": ["int"], "Default_option":"", "Default_value":"", "Possible":["int"],
             "Definition":"Integer value. Size of the vocabulary, i.e. maximum integer index + 1."}
Parameter_1 = {"Name":"output_dim", "Type": ["int"], "Default_option":"", "Default_value":"", "Possible":["int"],
             "Definition":"Integer value. Dimension of the dense embedding."}
Parameter_2 = {"Name":"embeddings_initializer","Type": ["option"], "Default_option":"uniform", "Default_value":"uniform", "Possible":Layer.getInitializers(),
             "Definition":"Initializer for the embeddings matrix (see keras.initializers)."}
Parameter_3 = {"Name":"embeddings_regularizer", "Type": ["option"], "Default_option":"None", "Default_value":"None", "Possible":Layer.getRegularizers(),
             "Definition":"Regularizer function applied to the embeddings matrix (see keras.regularizers)."}
#Parameter_4 = {"Name":"embedding_constraint", "Type": ["option"], "Default_option":"None", "Default_value":"None", "Possible":Layer.getConstraints(),
#             "Definition":"Constraint function applied to the embeddings matrix (see keras.constraints)."}
Parameter_4 = {"Name":"mask_zero", "Type": ["bool"], "Default_option":False, "Default_value":False, "Possible":[True,False],
             "Definition":"Boolean, whether or not the input value 0 is a special 'padding' value that should be masked out. This is useful when using recurrent layers which may take variable length input. If this is True, then all subsequent layers in the model need to support masking or an exception will be raised. If mask_zero is set to True, as a consequence, index 0 cannot be used in the vocabulary (input_dim should equal size of vocabulary + 1)."}
Parameter_5 = {"Name":"input_length", "Type": ["int"], "Default_option":"", "Default_value":"", "Possible":["int"],
             "Definition":"Length of input sequences, when it is constant. This argument is required if you are going to connect Flatten then Dense layers upstream (without it, the shape of the dense outputs cannot be computed)."}
## current optimizer doesnt support sparse. Can include once fixed.
#Parameter_6 = {"Name":"sparse", "Type": ["bool"], "Default_option":False, "Default_value":False, "Possible":[True,False],
#             "Definition":"If True, calling this layer returns a tf.SparseTensor. If False, the layer returns a dense tf.Tensor. For an entry with no features in a sparse tensor (entry with value 0), the embedding vector of index 0 is returned by default."}

Parameters = {"Parameter_0":Parameter_0, "Parameter_1":Parameter_1, "Parameter_2":Parameter_2, "Parameter_3":Parameter_3, 
              "Parameter_4":Parameter_4, "Parameter_5":Parameter_5}

list_Core.append(Layer.Layer(Name, Display_Name, Definition, Parameters))

## Masking Layer
Name = "Masking"
Display_Name = "Masking"
Definition = ["Masks a sequence by using a mask value to skip timesteps.\n\nFor each timestep in the input tensor (dimension #1 in the tensor), if all values in the input tensor at that timestep are equal to mask_value, then the timestep will be masked (skipped) in all downstream layers (as long as they support masking).\n\nIf any downstream layer does not support masking yet receives such an input mask, an exception will be raised."]
Parameter_0 = {"Name":"mask_value", "Type": ["float"], "Default_option":0.0, "Default_value":0.0, "Possible":["float"],
             "Definition":"If all values in the input tensor at that timestep are equal to mask_value, then the timestep will be masked (skipped)."}

Parameters = {"Parameter_0":Parameter_0}

list_Core.append(Layer.Layer(Name, Display_Name, Definition, Parameters))

## Lambda Layer
# skiped due to complexity to incorporate.
# this fuction allows the user to create a fuction to alter the input to an output defined by the fuction given the tensors.
# this would require to check tensors and function input as well as allow the user to create a any function. Thus being to complex.


def create_Layer(data, i):
    # get layer name
    layer = data["Layer_" + str(i)]

    # get the chosen settings for the layer
    Parameters = HyperParameters.getParameters(data["Layer_" + str(i)], list_Core)
    settings = HyperParameters.getSettings(data, Parameters, i, Layer.getName())

    new_layer = ""

    ## Input Object
    if layer == "Input":

        if settings["Parameter_1"] != None and settings["Parameter_2"] != None and settings["Parameter_3"] != None:
            shape = (settings["Parameter_0"], settings["Parameter_1"], settings["Parameter_2"], settings["Parameter_3"],)

        elif settings["Parameter_1"] != None and settings["Parameter_2"] != None:
            shape = (settings["Parameter_0"], settings["Parameter_1"], settings["Parameter_2"],)

        elif settings["Parameter_1"] != None:
            shape = (settings["Parameter_0"], settings["Parameter_1"],)

        elif settings["Parameter_1"] != None and settings["Parameter_2"] != None and settings["Parameter_3"] != None:
            shape = (settings["Parameter_0"], settings["Parameter_1"], settings["Parameter_2"], settings["Parameter_3"],)

        else:
            shape = (settings["Parameter_0"],)

        # Create the layer
        new_layer = keras.layers.InputLayer(input_shape=shape, batch_size=settings["Parameter_4"], name=settings["Parameter_5"], dtype=settings["Parameter_6"],
                                               sparse=settings["Parameter_7"], ragged=settings["Parameter_8"])
    
    ## Dense
    if layer == "Dense":

        new_layer = keras.layers.Dense(units=settings["Parameter_0"], activation=settings["Parameter_1"], use_bias=settings["Parameter_2"], kernel_initializer=settings["Parameter_3"],
                                       bias_initializer=settings["Parameter_4"], kernel_regularizer=settings["Parameter_5"], bias_regularizer=settings["Parameter_6"],
                                       activity_regularizer=settings["Parameter_7"])
     
    ## Activation
    if layer == "Activation":

        new_layer = keras.layers.Activation(activation=settings["Parameter_0"])

    ## Embedding Layer
    if layer == "Embedding":

        new_layer = keras.layers.Embedding(input_dim=settings["Parameter_0"], output_dim=settings["Parameter_1"], embeddings_initializer=settings["Parameter_2"],
                                           embeddings_regularizer=settings["Parameter_3"], mask_zero=settings["Parameter_4"], input_length=settings["Parameter_5"])

    ## Masking Layer
    if layer == "Masking":

        new_layer = keras.layers.Masking(mask_value=settings["Parameter_0"])

        
    return new_layer